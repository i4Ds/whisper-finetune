model:
  init_name: small
  bfloat16: False # If this is set, mixed_precision_training will use bfloat16
dataset:
  train_datasets: [i4ds/sg_corp_train_no_overlap_speaker_ret, i4ds/srg-full-train-val-v2, i4ds/mozilla-cv-13-long-text-de]
  select_n_per_t_ds: [1000, 1000, 1000] # How many rows to sample from the respective dataset per training run.
  groupby_col: [null, null, null] # If set, it will preform a groupby sample per column in the respective train dataset, similar to pandas groupby.sample but with training data. Total number of samples is select_n_per_t_ds * n_groups.
  val_datasets: [kenfus/67124e07520c24211abed261-raffaela.witting-claudia_28sec, kenfus/671f45f0520c24211abedbfa-n.ferrari-10min_28sec, i4ds/mozilla-cv-13-long-text-de]
  val_dataset_names: null # Optional: Custom names for validation datasets (for logging purposes)
  select_n_per_v_ds: [500, 500, 500] # How many rows to sample from the respective dataset per training run.
  train_split_name: "train" # Split name to use for the train dataset.
  valid_split_name: "validation" # Split name to use for the validation dataset.
  no_timestamp_training: False # Whether to use no timestamps for training.
  max_prompt_length: 223 # The maximum number of tokens to use for the prompt (Fixed by whisper).
  prompt_use_rate: 0.5 # How often to use the prompt.
  no_timestamp_rate: 0.5 # How often to use no timestamps.
  batch_size: 6
  batch_size_eval: 6
lr_scheduler:
  type: linear
  warmup_steps: 128 # If bigger than 1, then it's the total number of steps, else the ratio of total steps to use as warmup.
optimizer:
  type: adamw
  8bit: True
  params:
    lr: 2.0e-4
    weight_decay: 0.1
    betas: [0.9, 0.98]
    eps: 1.0e-6
    amsgrad: False
training:
  accum_grad_steps: 4
  label_smoothing: 0.05  # Label smoothing for cross-entropy loss
  train_only_decoder: False # If true, disable the grads of the encoder
  train_only_encoder: False # If true, disable the grads of the decoder
  max_grad_norm: 1.0
  stochastic_depth: 0.1 # Stochastic depth, used in whisper. See https://arxiv.org/abs/1603.09382
  epochs: 2
  eval_steps: 0.1 # % of each epoch to do validation.
  save_all_checkpoints: True
  upload_models_to_wandb: False # If final models should be uploaded to wandb.
  max_train_loss: 25 # We observed that if the loss goes above 25, the training run has failed and should be restarted with other parameters.
  mixed_precision_training: True
  mp_dtype: fp16
  gradient_checkpointing_encoder: True # If there should be gradient checkpointing in the encoder.
  gradient_checkpointing_encoder_last_only: False # Somehow, does not save a lot of memory (~1%).
  gradient_checkpointing_decoder: True # If there should be gradient checkpointing in the decoder.
augmentation: # Data augmentations, see the whisper paper for more details
  spec_augment:
    apply: True
    time_mask_param: 100
    p: 1.0
    freq_mask_param: 43
    time_warp_w: 80
  deep_spec_augment:
    apply: True
    time_mask_param: 100
    freq_mask_param: 27
    layer_indices: null # Which encoder layer to apply deep spec augment. If null, apply to all.
  bpe_dropout: 0.1 # Dropout on the BPE tokens.
  extremes_spec_augment:
    apply: False # Leads to unstable training.
    low_freq_range: 10
    high_freq_range: 20
  audio_augment: # Audio augmentations by reducing audio quality (white noise, high-pass filter, and low-pass filter).  See: https://github.com/daekeun-ml/azure-genai-utils/blob/main/azure_genai_utils/stt/augment.py
    apply_office_aug: True
    apply_baseline_aug: True
seed: 123
save_dir: output